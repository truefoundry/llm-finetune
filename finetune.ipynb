{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "print(\"#GPU Devices:\", torch.cuda.device_count())\n",
    "\n",
    "import bitsandbytes as _\n",
    "import flash_attn as _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Truefoundry (to save metrics, checkpoints and models!)\n",
    "You only need to do it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This should point to your Truefoundry platform endpoint\n",
    "TRUEFOUNDRY_HOST = os.getenv(\"TFY_HOST\", \"https://<your-org>.truefoundry.cloud\")\n",
    "\n",
    "import mlfoundry\n",
    "mlfoundry.login(TRUEFOUNDRY_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data\n",
    "\n",
    "Data needs to be in `jsonl` format with each line containing a json encoded string containing two keys `prompt` and `completion`\n",
    "\n",
    "```jsonl\n",
    "{\"prompt\": \"What is 2 + 2?\", \"completion\": \"The answer to 2 + 2 is 4\"}\n",
    "{\"prompt\": \"Flip a coin\", \"completion\": \"I flipped a coin and the result is heads!\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "...\n",
    "```\n",
    "\n",
    "In case you don't have data prepared, run the next cell to fetch the Stanford Alpaca dataset to use it as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget --progress=bar:force:noscroll https://assets.production.truefoundry.com/standford_alpaca_52k.jsonl -O standford_alpaca_52k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -2 standford_alpaca_52k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface hub model id to finetune e.g. \"stas/tiny-random-llama-2\"\n",
    "# If you created this notebook instance from Truefoundry's Model Catalogue, the model id will be set in the env\n",
    "model_id = os.getenv(\"TFY_FINETUNE_MODEL_ID\")\n",
    "\n",
    "# URI to training data. Can be a file on disk or an mlfoundry artifact fqn\n",
    "train_data = \"./standford_alpaca_52k.jsonl\"\n",
    "\n",
    "# URI to evaluation data. Can be a file on disk or an mlfoundry artifact fqn. \n",
    "eval_data = \"NA\"\n",
    "\n",
    "# When eval_data is set to \"NA\", use this portion of the train_data to use as eval\n",
    "eval_size = 0.1\n",
    "\n",
    "# How many samples to use for training. 0 means all data. Useful to test quickly\n",
    "max_num_samples = 100\n",
    "\n",
    "if not model_id:\n",
    "    print(\n",
    "        \"\"\"Warning! Variable `model_id` is not set. Please set it to some valid Huggingface hub model.\n",
    "        E.g model_id = \"stas/tiny-random-llama-2\"\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model to finetune:\", model_id)\n",
    "print(\"Train Data Location:\", train_data)\n",
    "print(\"Eval Data Location:\", eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lora Configuration\n",
    "\n",
    "# Enable LoRa with Quantization\n",
    "use_qlora = True\n",
    "\n",
    "# If you want to disable quantization, set `use_qlora` to False and set `use_lora` to True\n",
    "use_lora = False\n",
    "\n",
    "# Which modules to target for qlora. \"auto\" targets all linear layers (execpt embeddings, lm_head). To customize give a json encoded list\n",
    "lora_target_modules = \"auto\"\n",
    "\n",
    "# qlora r. Increasing this will increase GPU memory requirement and training time but can give better results\n",
    "lora_r = 32\n",
    "\n",
    "# qlora alpha\n",
    "lora_alpha = max(16, 2 * lora_r)\n",
    "\n",
    "# qlora dropout\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# qlora bias\n",
    "lora_bias = \"none\"\n",
    "\n",
    "\n",
    "if use_qlora and use_lora:\n",
    "    raise ValueError(\"Both `use_qlora` and `use_lora` cannot be True at the same time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Arguments\n",
    "\n",
    "# Where to dump checkpoints and model\n",
    "output_dir = \"./model\"\n",
    "\n",
    "# If to delete `output_dir` before starting\n",
    "cleanup_output_dir_on_start = False\n",
    "\n",
    "# If to also train on prompt. Setting this to True will increase GPU memory requirement and training time\n",
    "train_on_prompt = False\n",
    "\n",
    "# Max batch size per GPU. Increasing this will increase GPU memory requirement and training time\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# How many epochs to run training for\n",
    "num_train_epochs = 5\n",
    "\n",
    "# How many eval steps to wait for the eval loss to improve before stopping\n",
    "early_stopping_patience = 10\n",
    "\n",
    "# How much the eval loss should improve at least to not count towards early stopping\n",
    "early_stopping_threshold = \"0.0\"\n",
    "\n",
    "# If to resume from checkpoints when available\n",
    "resume_from_checkpoint = True\n",
    "\n",
    "# Gradient accumulation steps\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Mixed Precision Training. We automatically select the precision based on GPU capability\n",
    "mixed_precision = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "bf16 = (mixed_precision == \"bf16\")\n",
    "fp16 = (mixed_precision == \"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often to log metrics. Value less than 1 denotes every X% of total run\n",
    "logging_steps = 5\n",
    "\n",
    "# How often to evaluate. Value less than 1 denotes every X% of total run\n",
    "eval_steps = 0.05\n",
    "\n",
    "# How often to save checkpoints. Value less than 1 denotes every X% of total run\n",
    "save_steps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfoundry_utils import generate_run_name, get_or_create_run\n",
    "\n",
    "# Enable reporting metrics to mlfoundry\n",
    "mlfoundry_enable_reporting = True\n",
    "\n",
    "# Which ML Repo to log metrics and checkpoints to. \n",
    "# You can create new ML Repos from the https://<your-org>.truefoundry.cloud/mlfoundry page\n",
    "# Docs: https://docs.truefoundry.com/docs/key-concepts#creating-a-ml-repo\n",
    "mlfoundry_ml_repo = \"llm-finetuning\"\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "mlfoundry_log_checkpoints = True\n",
    "\n",
    "# Run to which metrics and checkpoints will be logged\n",
    "mlfoundry_run_name = generate_run_name(model_id)\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "mlfoundry_checkpoint_artifact_name = f\"ckpt-{mlfoundry_run_name}\"\n",
    "\n",
    "\n",
    "if mlfoundry_enable_reporting:\n",
    "    print(f\"Checkpoints will be logged with name {mlfoundry_checkpoint_artifact_name}\")\n",
    "    get_or_create_run(\n",
    "        ml_repo=mlfoundry_ml_repo,\n",
    "        run_name=mlfoundry_run_name,\n",
    "        auto_end=False,\n",
    "        create_ml_repo=True\n",
    "    )\n",
    "    print(f\"You can click on the above link to track metrics and checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mlfoundry_enable_reporting:\n",
    "    import os\n",
    "    from urllib.parse import urljoin\n",
    "    from tensorboard import notebook\n",
    "\n",
    "    tb_logs = os.path.join(output_dir, \"runs\")\n",
    "    os.makedirs(tb_logs, exist_ok=True)\n",
    "    os.environ[\"TENSORBOARD_PROXY_URL\"] = urljoin(os.getenv(\"NB_PREFIX\", \"/\"), \"proxy/%PORT%/\")\n",
    "    notebook.start(f\"--logdir {tb_logs} --reload_interval 30.0 --reload_multifile True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMAND = f\"\"\"\n",
    "accelerate launch \\\n",
    "--mixed_precision {mixed_precision} \\\n",
    "train.py \\\n",
    "--use_ddp true \\\n",
    "--output_dir {output_dir} \\\n",
    "--model_id {model_id} \\\n",
    "--train_data {train_data} \\\n",
    "--eval_data {eval_data} \\\n",
    "--eval_size {eval_size} \\\n",
    "--max_num_samples {max_num_samples} \\\n",
    "--bf16 {bf16} \\\n",
    "--fp16 {fp16} \\\n",
    "--num_train_epochs {num_train_epochs} \\\n",
    "--per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "--per_device_eval_batch_size {per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--train_on_prompt {train_on_prompt} \\\n",
    "--logging_strategy steps \\\n",
    "--logging_steps {logging_steps} \\\n",
    "--evaluation_strategy steps \\\n",
    "--eval_steps {eval_steps} \\\n",
    "--save_strategy steps \\\n",
    "--save_steps {save_steps} \\\n",
    "--use_qlora {use_qlora} \\\n",
    "--use_lora {use_lora} \\\n",
    "--qlora_bit_length 4 \\\n",
    "--lora_target_modules {lora_target_modules} \\\n",
    "--lora_r {lora_r} \\\n",
    "--lora_alpha {lora_alpha} \\\n",
    "--lora_dropout {lora_dropout} \\\n",
    "--lora_bias {lora_bias} \\\n",
    "--mlfoundry_enable_reporting {mlfoundry_enable_reporting} \\\n",
    "--mlfoundry_ml_repo {mlfoundry_ml_repo} \\\n",
    "--mlfoundry_run_name {mlfoundry_run_name} \\\n",
    "--mlfoundry_checkpoint_artifact_name {mlfoundry_checkpoint_artifact_name} \\\n",
    "--mlfoundry_log_checkpoints {mlfoundry_log_checkpoints} \\\n",
    "--cleanup_output_dir_on_start {cleanup_output_dir_on_start}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Command to run: {COMMAND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{COMMAND}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-jupyter-base",
   "language": "python",
   "name": "conda-env-.conda-jupyter-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
