{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Truefoundry (to save metrics, checkpoints and models!)\n",
    "You only need to do it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This should point to your Truefoundry platform endpoint\n",
    "TRUEFOUNDRY_HOST = os.getenv(\"TFY_HOST\", \"https://<your-org>.truefoundry.cloud\")\n",
    "\n",
    "import mlfoundry\n",
    "mlfoundry.login(TRUEFOUNDRY_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be in `jsonl` format with each line containing a json encoded string containing two keys `prompt` and `completion`\n",
    "\n",
    "```jsonl\n",
    "{\"prompt\": \"What is 2 + 2?\", \"completion\": \"The answer to 2 + 2 is 4\"}\n",
    "{\"prompt\": \"Flip a coin\", \"completion\": \"I flipped a coin and the result is heads!\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "...\n",
    "```\n",
    "\n",
    "Once you have your data on `.jsonl` files, you can upload them to the file tree on the left and change the `train_data` and `eval_data` variables in the `Data Parameters` section\n",
    "\n",
    "---\n",
    "In case you don't have data prepared, run the next cell to fetch the [Meta LIMA Dataset](https://arxiv.org/abs/2305.11206) (https://huggingface.co/datasets/GAIR/lima) to use it as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget --progress=bar:force:noscroll https://assets.production.truefoundry.com/lima_llama2_1k.jsonl -O lima_llama2_1k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -2 lima_llama2_1k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Only first 100 data points will be used. This is okay for quick testing. To use all data points please set `max_num_samples` to 0\n"
     ]
    }
   ],
   "source": [
    "# URI to training data. Can be a file on disk or an mlfoundry artifact fqn\n",
    "train_data = \"./lima_llama2_1k.jsonl\"\n",
    "\n",
    "# URI to evaluation data. Can be a file on disk or an mlfoundry artifact fqn. \n",
    "eval_data = \"NA\"\n",
    "\n",
    "# When eval_data is set to \"NA\", use this portion of the train_data to use as eval\n",
    "eval_size = 0.1\n",
    "\n",
    "# How many samples to use for training. 0 means all data. Useful to test quickly\n",
    "max_num_samples = 0\n",
    "\n",
    "if max_num_samples != 0:\n",
    "    print(f\"Note: Only first {max_num_samples} data points will be used. This is okay for quick testing. To use all data points please set `max_num_samples` to 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconfigured Parameters\n",
    "This section loads the default parameters configured when deploying the notebook such as the model id, batch size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_launch_parameters\n",
    "\n",
    "launch_parameters = load_launch_parameters(\"./finetune-config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Huggingface hub model id to finetune e.g. \"stas/tiny-random-llama-2\"\n",
    "# If you created this notebook instance from Truefoundry's Model Catalogue, the model id will be set in `launch_parameters`\n",
    "model_id = launch_parameters.model_id\n",
    "\n",
    "if not model_id:\n",
    "    print('Warning! Variable `model_id` is not set. Please set it to some valid Huggingface hub model. E.g model_id = \"stas/tiny-random-llama-2\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRa Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRa with Quantization\n",
    "use_qlora = True\n",
    "\n",
    "# If you want to disable quantization, set `use_qlora` to False and set `use_lora` to True\n",
    "use_lora = False\n",
    "\n",
    "# qlora r. Increasing this will increase GPU memory requirement and training time but can give better results\n",
    "lora_r = 32\n",
    "\n",
    "# qlora alpha\n",
    "lora_alpha = max(16, 2 * lora_r)\n",
    "\n",
    "\n",
    "if use_qlora and use_lora:\n",
    "    raise ValueError(\"Both `use_qlora` and `use_lora` cannot be True at the same time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to dump checkpoints and model\n",
    "output_dir = \"./model\"\n",
    "\n",
    "# If to delete `output_dir` before starting\n",
    "cleanup_output_dir_on_start = False\n",
    "\n",
    "# Max Sequence Length. \n",
    "# Increasing this will allow longer sequences but will significantly increase GPU memory requirement and training time.\n",
    "# This cannot be greater than model's max sequence length\n",
    "max_length = launch_parameters.max_length\n",
    "\n",
    "# Max batch size per GPU. \n",
    "# Increasing this will increase GPU memory requirement and training time\n",
    "per_device_train_batch_size = launch_parameters.batch_size\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.00003\n",
    "\n",
    "# How many epochs to run training for\n",
    "num_train_epochs = 10\n",
    "\n",
    "# How often to evaluate. Value less than 1 denotes every X% of total run\n",
    "eval_steps = 0.05\n",
    "\n",
    "# How often to save checkpoints. Value less than 1 denotes every X% of total run\n",
    "save_steps = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfoundry_utils import generate_run_name, get_or_create_run\n",
    "\n",
    "# Enable reporting metrics to mlfoundry\n",
    "mlfoundry_enable_reporting = True\n",
    "\n",
    "# Which ML Repo to log metrics and checkpoints to. \n",
    "# You can create new ML Repos from the https://<your-org>.truefoundry.cloud/mlfoundry page\n",
    "# Docs: https://docs.truefoundry.com/docs/key-concepts#creating-a-ml-repo\n",
    "mlfoundry_ml_repo = \"llm-finetuning\"\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "mlfoundry_log_checkpoints = True\n",
    "\n",
    "# Run to which metrics and checkpoints will be logged\n",
    "mlfoundry_run_name = generate_run_name(model_id)\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "mlfoundry_checkpoint_artifact_name = f\"ckpt-{mlfoundry_run_name}\"\n",
    "\n",
    "\n",
    "if mlfoundry_enable_reporting:\n",
    "    print(f\"Checkpoints will be logged with name {mlfoundry_checkpoint_artifact_name}\")\n",
    "    get_or_create_run(\n",
    "        ml_repo=mlfoundry_ml_repo,\n",
    "        run_name=mlfoundry_run_name,\n",
    "        auto_end=False,\n",
    "        create_ml_repo=True\n",
    "    )\n",
    "    print(\"You can click on the above link to track metrics and checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _launch_tensorboard():\n",
    "    import os\n",
    "    from urllib.parse import urljoin\n",
    "    from tensorboard import notebook\n",
    "\n",
    "    tb_logs = os.path.join(\".\", \"tensorboard_logs\")\n",
    "    os.makedirs(tb_logs, exist_ok=True)\n",
    "    os.environ[\"TENSORBOARD_PROXY_URL\"] = urljoin(os.getenv(\"NB_PREFIX\", \"/\"), \"proxy/%PORT%/\")\n",
    "    notebook.start(f\"--logdir {tb_logs} --reload_interval 30.0 --reload_multifile True\")\n",
    "\n",
    "if not mlfoundry_enable_reporting:\n",
    "    _launch_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Precision Training. We automatically select the precision based on GPU capability\n",
    "mixed_precision = \"bf16\" if torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "bf16 = (mixed_precision == \"bf16\")\n",
    "fp16 = (mixed_precision == \"fp16\")\n",
    "\n",
    "COMMAND = f\"\"\"\n",
    "accelerate launch \\\n",
    "--mixed_precision {mixed_precision} \\\n",
    "--use_deepspeed \\\n",
    "train.py \\\n",
    "--deepspeed ./3_ds_z2_config.json \\\n",
    "--bf16 {bf16} \\\n",
    "--fp16 {fp16} \\\n",
    "--model_id {model_id} \\\n",
    "--output_dir {output_dir} \\\n",
    "--train_data {train_data} \\\n",
    "--eval_data {eval_data} \\\n",
    "--eval_size {eval_size} \\\n",
    "--max_num_samples {max_num_samples} \\\n",
    "--train_on_prompt False \\\n",
    "--max_length {max_length} \\\n",
    "--use_qlora {use_qlora} \\\n",
    "--use_lora {use_lora} \\\n",
    "--qlora_bit_length 4 \\\n",
    "--lora_target_modules auto \\\n",
    "--lora_r {lora_r} \\\n",
    "--lora_alpha {lora_alpha} \\\n",
    "--lora_dropout 0.05 \\\n",
    "--lora_bias none \\\n",
    "--num_train_epochs {num_train_epochs} \\\n",
    "--early_stopping_patience 10 \\\n",
    "--early_stopping_threshold 0.0 \\\n",
    "--auto_find_batch_size false \\\n",
    "--per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "--per_device_eval_batch_size {per_device_train_batch_size} \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--logging_strategy steps \\\n",
    "--logging_steps 5 \\\n",
    "--evaluation_strategy steps \\\n",
    "--eval_steps {eval_steps} \\\n",
    "--save_strategy steps \\\n",
    "--save_steps {save_steps} \\\n",
    "--mlfoundry_enable_reporting {mlfoundry_enable_reporting} \\\n",
    "--mlfoundry_ml_repo {mlfoundry_ml_repo} \\\n",
    "--mlfoundry_run_name {mlfoundry_run_name} \\\n",
    "--mlfoundry_checkpoint_artifact_name {mlfoundry_checkpoint_artifact_name} \\\n",
    "--mlfoundry_log_checkpoints {mlfoundry_log_checkpoints} \\\n",
    "--cleanup_output_dir_on_start False \\\n",
    "--resume_from_checkpoint True\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Command to run: {COMMAND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{COMMAND} | tee train.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
