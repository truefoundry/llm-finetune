{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Truefoundry (to save metrics, checkpoints and models!)\n",
    "You only need to do it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This should point to your Truefoundry platform endpoint\n",
    "TRUEFOUNDRY_HOST = os.getenv(\"TFY_HOST\", \"https://<your-org>.truefoundry.cloud\")\n",
    "\n",
    "import truefoundry\n",
    "truefoundry.login(TRUEFOUNDRY_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We support two different data formats:\n",
    "\n",
    "### `Chat`\n",
    "\n",
    "Data needs to be in `jsonl` format with each line containing a whole conversation in OpenAI chat format i.e. each line contains a key called `messages`. Each `messages` key contains a list of messages, where each message is a dictionary with `role` and `content` keys. The `role` key can be either `user`, `assistant` or `system` and the `content` key contains the message content.\n",
    "\n",
    "```jsonl\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "Reference: https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "\n",
    "### `Completion`\n",
    "Data needs to be in `jsonl` format with each line containing a json encoded string containing two keys `prompt` and `completion`.\n",
    "\n",
    "```jsonl\n",
    "{\"prompt\": \"What is 2 + 2?\", \"completion\": \"The answer to 2 + 2 is 4\"}\n",
    "{\"prompt\": \"Flip a coin\", \"completion\": \"I flipped a coin and the result is heads!\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "...\n",
    "```\n",
    "\n",
    "Reference: https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "### Uploading data to notebook\n",
    "\n",
    "Once you have your data on `.jsonl` files, you can upload them to the file tree on the left and change the `train_data_uri` and `eval_data_uri` variables in the `Data Parameters` section\n",
    "\n",
    "![Upload Data](./assets/upload-data.png)\n",
    "\n",
    "---\n",
    "\n",
    "In case you don't have data prepared, run the next cell to fetch the [Chat Alpaca English Dataset](https://github.com/cascip/ChatAlpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Chat Type Data\n",
    "wget --progress=bar:force:noscroll https://assets.production.truefoundry.com/chatalpaca-openai-10k.jsonl -O chatalpaca-openai-10k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -2 chatalpaca-openai-10k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion Type Data: https://huggingface.co/datasets/tatsu-lab/alpaca\n",
    "# wget --progress=bar:force:noscroll https://assets.production.truefoundry.com/standford_alpaca_train_49k.jsonl -O standford_alpaca_train_49k.jsonl\n",
    "# wget --progress=bar:force:noscroll https://assets.production.truefoundry.com/standford_alpaca_test_2k.jsonl -O standford_alpaca_test_2k.jsonl\n",
    "# head -2 standford_alpaca_train_49k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "from data_utils import DatasetType\n",
    "\n",
    "# Type of dataset - Either `completion` or `chat`\n",
    "dataset_type = DatasetType.chat.value\n",
    "\n",
    "# URI to training data. Can be a file on disk or an mlfoundry artifact fqn\n",
    "train_data_uri: str = \"./chatalpaca-openai-10k.jsonl\"\n",
    "\n",
    "# URI to evaluation data. Can be a file on disk or an mlfoundry artifact fqn. \n",
    "# Set to \"None\" if you want to split from train data\n",
    "eval_data_uri: Optional[str] = None\n",
    "\n",
    "# When eval_data is set to `None`, use this portion of the train_data to use as eval\n",
    "eval_size = 0.1\n",
    "\n",
    "# If your dataset is small (< 50 examples), set this to False\n",
    "sample_packing = True\n",
    "\n",
    "# How many steps to use for training. None means all data. Useful to test quickly\n",
    "max_steps: Optional[int] = None\n",
    "\n",
    "if max_steps is not None:\n",
    "    print(f\"Note: max_steps is set, this might not use the entire training data. This is okay for quick testing. To use all data points please set `max_steps` to `None`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconfigured Parameters\n",
    "This section loads the default parameters configured when deploying the notebook such as the model id, batch size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_launch_parameters\n",
    "\n",
    "launch_parameters = load_launch_parameters(\"/mnt/llm-finetune/finetune-config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface hub model id to finetune e.g. \"stas/tiny-random-llama-2\"\n",
    "# If you created this notebook instance from Truefoundry's Model Catalogue, the model id will be set in `launch_parameters`\n",
    "model_id = launch_parameters.model_id\n",
    "\n",
    "if not model_id:\n",
    "    print('Warning! Variable `model_id` is not set. Please set it to some valid Huggingface hub model. E.g model_id = \"stas/tiny-random-llama-2\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRa Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = \"qlora\"\n",
    "\n",
    "# lora r. Increasing this will increase GPU memory requirement and training time but can give better results\n",
    "lora_r = 32\n",
    "\n",
    "# lora alpha\n",
    "lora_alpha = max(16, 2 * lora_r)\n",
    "\n",
    "# Whether to apply Lora to all linear layers\n",
    "lora_target_linear = True\n",
    "\n",
    "#  The names of the modules to apply Lora to. These will be added to modules found by `lora_target_linear` if that is enabled\n",
    "lora_target_modules: Optional[List[str]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to dump checkpoints and model\n",
    "output_dir = \"./outputs\"\n",
    "\n",
    "# If to delete `output_dir` before starting\n",
    "cleanup_output_dir_on_start = False\n",
    "\n",
    "# Max Sequence Length. \n",
    "# Increasing this will allow longer sequences but will significantly increase GPU memory requirement and training time.\n",
    "# This cannot be greater than model's max sequence length\n",
    "max_sequence_length = launch_parameters.max_length\n",
    "\n",
    "# If to drop sequences that are longer than max_sequence_length\n",
    "# error ->  will raise an error that are longer than max_sequence_length\n",
    "# truncate -> will truncate sequences that are longer than max_sequence_length\n",
    "# drop -> will drop sequences that are longer than max_sequence_length\n",
    "long_sequences_strategy = \"error\"\n",
    "\n",
    "# Batch size per GPU. \n",
    "# Increasing this will increase GPU memory requirement and training time\n",
    "micro_batch_size = launch_parameters.batch_size\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# How many epochs to run training for\n",
    "num_epochs = 10\n",
    "\n",
    "# How often to evaluate. Value less than 1 denotes every X% of total run\n",
    "eval_steps = 0.1\n",
    "\n",
    "# How often to save checkpoints. Value less than 1 denotes every X% of total run\n",
    "save_steps = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlfoundry_utils import generate_run_name, get_or_create_run\n",
    "\n",
    "# Enable reporting metrics to mlfoundry\n",
    "truefoundry_ml_enable_reporting = True\n",
    "\n",
    "# Which ML Repo to log metrics and checkpoints to. \n",
    "# You can create new ML Repos from the https://<your-org>.truefoundry.cloud/mlfoundry page\n",
    "# Docs: https://docs.truefoundry.com/docs/key-concepts#creating-a-ml-repo\n",
    "truefoundry_ml_repo = \"llm-finetuning\"\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "truefoundry_ml_log_checkpoints = True\n",
    "\n",
    "# Run to which metrics and checkpoints will be logged\n",
    "truefoundry_ml_run_name = generate_run_name(model_id, seed=os.getpid())\n",
    "\n",
    "# If to upload checkpoints to ML Repo when they are saved\n",
    "truefoundry_ml_checkpoint_artifact_name = f\"ckpt-{truefoundry_ml_run_name}\"\n",
    "\n",
    "\n",
    "if truefoundry_ml_enable_reporting:\n",
    "    print(f\"Checkpoints will be logged with name {truefoundry_ml_checkpoint_artifact_name}\")\n",
    "    get_or_create_run(\n",
    "        ml_repo=truefoundry_ml_repo,\n",
    "        run_name=truefoundry_ml_run_name,\n",
    "        auto_end=False,\n",
    "    )\n",
    "    print(\"You can click on the above link to track metrics and checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _launch_tensorboard():\n",
    "    import os\n",
    "    from urllib.parse import urljoin\n",
    "    from tensorboard import notebook\n",
    "\n",
    "    tb_logs = os.path.join(os.path.abspath(output_dir), \"model\", \"runs\")\n",
    "    os.makedirs(tb_logs, exist_ok=True)\n",
    "    os.environ[\"TENSORBOARD_PROXY_URL\"] = urljoin(os.getenv(\"NB_PREFIX\", \"/\"), \"proxy/%PORT%/\")\n",
    "    notebook.start(f\"--logdir {tb_logs} --reload_interval 30.0 --reload_multifile True\")\n",
    "\n",
    "if not truefoundry_ml_enable_reporting:\n",
    "    _launch_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Mixed Precision Training. We automatically select the precision based on GPU capability\n",
    "is_ampere_or_newer = torch.cuda.get_device_capability(device=0) >= (8, 0)\n",
    "mixed_precision = \"bf16\" if is_ampere_or_newer and torch.cuda.is_bf16_supported() else \"fp16\"\n",
    "\n",
    "COMMAND = f\"\"\"\n",
    "accelerate launch \\\n",
    "--mixed_precision {mixed_precision} \\\n",
    "--use_deepspeed \\\n",
    "train.py \\\n",
    "config-base.yaml \\\n",
    "--deepspeed ./deepspeed_configs/3_ds_z2_config.json \\\n",
    "--gradient_checkpointing unsloth \\\n",
    "--base_model {model_id} \\\n",
    "--output_dir {output_dir} \\\n",
    "--dataset_type {dataset_type} \\\n",
    "--train_data_uri {train_data_uri} \\\n",
    "--val_data_uri {eval_data_uri} \\\n",
    "--val_set_size {eval_size} \\\n",
    "--max_steps {max_steps} \\\n",
    "--sequence_len {max_sequence_length} \\\n",
    "--long_sequences_strategy {long_sequences_strategy} \\\n",
    "--train_on_inputs False \\\n",
    "--sample_packing {sample_packing} \\\n",
    "--pad_to_sequence_len True \\\n",
    "--num_epochs {num_epochs} \\\n",
    "--micro_batch_size {micro_batch_size} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--early_stopping_patience 10 \\\n",
    "--adapter qlora \\\n",
    "--lora_target_linear {lora_target_linear} \\\n",
    "--lora_target_modules {lora_target_modules} \\\n",
    "--lora_r {lora_r} \\\n",
    "--lora_alpha {lora_alpha} \\\n",
    "--lora_dropout 0.05 \\\n",
    "--logging_steps 5 \\\n",
    "--evaluation_strategy steps \\\n",
    "--eval_steps {eval_steps} \\\n",
    "--save_strategy steps \\\n",
    "--save_steps {save_steps} \\\n",
    "--seed 42 \\\n",
    "--truefoundry_ml_enable_reporting {truefoundry_ml_enable_reporting} \\\n",
    "--truefoundry_ml_repo {truefoundry_ml_repo} \\\n",
    "--truefoundry_ml_run_name {truefoundry_ml_run_name} \\\n",
    "--truefoundry_ml_checkpoint_artifact_name {truefoundry_ml_checkpoint_artifact_name} \\\n",
    "--truefoundry_ml_log_checkpoints {truefoundry_ml_log_checkpoints} \\\n",
    "--cleanup_output_dir_on_start {cleanup_output_dir_on_start} \\\n",
    "--resume_from_checkpoint True \\\n",
    "| tee train.log\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Command to run: {COMMAND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{COMMAND}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
