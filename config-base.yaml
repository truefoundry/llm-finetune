# Mostly set externally explicitly
adapter: qlora
base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
debug: False
micro_batch_size: 1
revision_of_model:
sequence_len: 2048
val_set_size: 0.1
## Added by TrueFoundry, not native to Axolotl
train_data_uri: null
val_data_uri: null
dataset_type: completion  # Can be completion | chat
mlfoundry_enable_reporting: False
mlfoundry_ml_repo: null

# ---------------------
# Auto computed and set by script based on environment and external state
# Only edit them if you know what you are doing
data_dir: auto # type: string
datasets: auto # type: list
test_datasets: auto # type: list
bf16: auto # type: bool
bfloat16: auto # type: bool
flash_attn_fuse_mlp: auto # type: bool
flash_attn_fuse_qkv: auto # type: bool
float16: auto # type: bool
fp16: auto # type: bool
load_in_4bit: auto # type: bool
lora_modules_to_save: auto # type: list
resume_from_checkpoint: auto # type: bool
special_tokens: auto  # type: dict
tf32: auto # type: bool
## Added by TrueFoundry, not native to Axolotl
mlfoundry_run_name: auto # type: string
mlfoundry_checkpoint_artifact_name: auto # type: string


# ---------------------
# Defaults
bnb_config_kwargs:
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: True
  llm_int8_has_fp16_weight: False
adam_beta1: 0.9
adam_beta2: 0.99
auto_resume_from_checkpoints: False
base_model_ignore_patterns:
  - '*.h5'
  - '*.ot'
  - '*.tflite'
  - '*.msgpack'
chat_template: chatml
dataset_prepared_path: ./outputs/data/last_run_prepared
ddp_timeout: 21600
deepspeed: ./deepspeed_configs/3_ds_z2_config.json
default_system_message: You are a helpful assistant. Please give a long and detailed answer.
early_stopping_patience: 10
eval_sample_packing: False
eval_steps: 0.1
eval_strategy: steps
flash_attention: True
flash_attn_cross_entropy: True
flash_attn_rms_norm: True
gradient_accumulation_steps: 4
gradient_checkpointing: True
learning_rate: 0.00001
load_best_model_at_end: True
load_in_8bit: False
logging_steps: 5
lora_alpha: 64
lora_dropout: 0.05
lora_on_cpu: False
lora_r: 32
lora_target_modules: null
lora_target_linear: True
lr_scheduler: cosine
max_grad_norm: 1.0
type_of_model: AutoModelForCausalLM
num_epochs: 10
optimizer: adamw_torch_fused
output_dir: ./outputs
pad_to_sequence_len: True
report_to: tensorboard
resize_token_embeddings_to_32x: False
sample_packing: True
save_safetensors: True
save_steps: 0.1
save_strategy: steps
save_total_limit: 1
seed: 42
strict: False
tokenizer_type: AutoTokenizer
train_on_inputs: False
trust_remote_code: True
warmup_ratio: 0.1
weight_decay: 0.01
## Added by TrueFoundry, not native to Axolotl
cleanup_output_dir_on_start: False
logging_dir: ./tensorboard_logs
mlfoundry_log_checkpoints: True
use_mflow: False
use_wandb: False
